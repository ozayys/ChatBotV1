import OpenAI from 'openai';
import axios from 'axios';

// OpenAI client configuration
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Custom model configuration
const CUSTOM_MODEL_URL = process.env.CUSTOM_MODEL_URL || 'http://localhost:8000/chat';
const MISTRAL_MODEL_URL = process.env.MISTRAL_MODEL_URL || 'http://localhost:8002/chat';

interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface AIResponse {
  content: string;
  model: string;
  usage?: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

export class AIService {
  // OpenAI API kullanarak cevap al
  static async getOpenAIResponse(
    message: string, 
    previousMessages: ChatMessage[] = [],
    isMathRelated: boolean = false
  ): Promise<AIResponse> {
    try {
      // System message'Ä± matematik odaklÄ± yap
      const systemMessage: ChatMessage = {
        role: 'system',
        content: isMathRelated 
          ? 'You are a helpful AI assistant specialized in mathematics. Provide clear, step-by-step solutions to mathematical problems and explain concepts thoroughly.'
          : 'You are a helpful AI assistant. Provide clear, accurate, and helpful responses to user questions.'
      };

      // Mesaj geÃ§miÅŸini oluÅŸtur
      const messages: ChatMessage[] = [
        systemMessage,
        ...previousMessages.slice(-10), // Son 10 mesajÄ± al (token limiti iÃ§in)
        { role: 'user', content: message }
      ];

      const completion = await openai.chat.completions.create({
        model: 'gpt-3.5-turbo',
        messages: messages,
        max_tokens: 1000,
        temperature: 0.7,
      });

      const response = completion.choices[0]?.message?.content || 'Sorry, I could not generate a response.';

      return {
        content: response,
        model: 'gpt-3.5-turbo',
        usage: {
          prompt_tokens: completion.usage?.prompt_tokens || 0,
          completion_tokens: completion.usage?.completion_tokens || 0,
          total_tokens: completion.usage?.total_tokens || 0,
        }
      };
    } catch (error) {
      console.error('OpenAI API Error:', error);
      throw new Error('OpenAI API request failed. Please check your API key and try again.');
    }
  }

  // Custom model kullanarak cevap al
  static async getCustomModelResponse(
    message: string, 
    isMathRelated: boolean = false
  ): Promise<AIResponse> {
    try {
      console.log('ğŸ¤– Trying FLAN-T5-CodeParrot Model for message:', message);
      
      // T5 fine-tuned modelini kullan
      const result = await this.callT5Model(message);
      
      console.log('âœ… CodeParrot Model Response received:', result.model);
      return result;
      
          } catch (error) {
        console.error('âŒ CodeParrot Model Error:', error);
        console.log('ğŸ”„ Falling back to demo response while CodeParrot service is starting...');
      
      // CodeParrot model servisi henÃ¼z baÅŸlamadÄ±ysa demo response ver
      return {
        content: `ğŸ¤– **CodeParrot Model Demo Response:**\n\nFLAN-T5-CodeParrot modeliniz henÃ¼z yÃ¼kleniyor veya baÅŸlatÄ±lmadÄ±.\n\n**MesajÄ±nÄ±z:** "${message}"\n\n**Demo YanÄ±t:** Bu, CodeParrot modelinizin yerini tutan geÃ§ici bir yanÄ±ttÄ±r. GerÃ§ek CodeParrot modelinizi kullanmak iÃ§in:\n\n1. Model servisini baÅŸlatÄ±n: \`model_service/start_model.bat\`\n2. Port 8000'de servisiyi Ã§alÄ±ÅŸtÄ±rÄ±n\n3. Bu mesajÄ± tekrar gÃ¶nderin\n\n*Bu demo yanÄ±tÄ± CodeParrot model servisi Ã§alÄ±ÅŸmaya baÅŸladÄ±ÄŸÄ±nda otomatik olarak gerÃ§ek model yanÄ±tlarÄ±yla deÄŸiÅŸecektir.*`,
        model: 'CodeParrot-Demo-Fallback'
      };
    }
  }

  // T5 Fine-tuned modelini Ã§aÄŸÄ±r
  private static async callT5Model(message: string): Promise<AIResponse> {
    try {
      console.log(`ğŸ”— Making request to T5 model at: ${CUSTOM_MODEL_URL}`);
      console.log(`ğŸ“ Message: "${message}"`);
      
      const response = await axios.post(CUSTOM_MODEL_URL, {
        message: message,
        max_length: 512
      }, {
        timeout: 30000, // 30 second timeout
        headers: {
          'Content-Type': 'application/json'
        }
      });

      console.log('ğŸ“¡ T5 Model Response Status:', response.status);
      console.log('ğŸ“¡ T5 Model Response Data:', response.data);

      if (response.data && response.data.response) {
        const result = {
          content: response.data.response,
          model: response.data.model || 'T5-Fine-tuned'
        };
        
        console.log('âœ… Successfully processed T5 response:', result.model);
        return result;
      } else {
        throw new Error('Invalid response structure from T5 model service');
      }
    } catch (error) {
      if (axios.isAxiosError(error)) {
        if (error.code === 'ECONNREFUSED') {
          throw new Error('ğŸš« T5 model service is not running on port 8000. Please start: model_service/start_model.bat');
        } else if (error.response) {
          console.log('âŒ T5 Service Error Response:', error.response.data);
          throw new Error(`T5 service error (${error.response.status}): ${error.response.data?.error || error.message}`);
        } else if (error.request) {
          throw new Error('ğŸŒ Network timeout - T5 model service not responding');
        } else {
          throw new Error(`Request setup error: ${error.message}`);
        }
      }
      throw error;
    }
  }

  // Mistral 7B model kullanarak cevap al
  static async getMistralResponse(
    message: string, 
    previousMessages: ChatMessage[] = []
  ): Promise<AIResponse> {
    try {
      console.log('ğŸš€ Trying Mistral 7B Model for message:', message);
      
      // Mistral 7B modelini kullan
      const result = await this.callMistralModel(message);
      
      console.log('âœ… Mistral 7B Model Response received:', result.model);
      return result;
      
    } catch (error) {
      console.error('âŒ Mistral 7B Model Error - FULL ERROR:', error);
      
      // Daha detaylÄ± hata log'u
      if (error instanceof Error) {
        console.error('Error message:', error.message);
        console.error('Error stack:', error.stack);
      }
      
      // Network error ise Ollama servisini kontrol et
      if (error instanceof Error && error.message.includes('Network error')) {
        console.log('ğŸ”„ Network error detected, trying to restart Ollama connection...');
        
        // 2 saniye bekle ve tekrar dene
        await new Promise(resolve => setTimeout(resolve, 2000));
        
        try {
          console.log('ğŸ”„ Retrying Mistral request...');
          const retryResult = await this.callMistralModel(message);
          console.log('âœ… Mistral retry successful!');
          return retryResult;
        } catch (retryError) {
          console.error('âŒ Mistral retry failed:', retryError);
        }
      }
      
      console.log('ğŸ”„ Falling back to demo response...');
    
      // Mistral model servisi henÃ¼z baÅŸlamadÄ±ysa demo response ver
      return {
        content: `ğŸš€ **Mistral 7B Model Demo Response:**\n\nMistral 7B modeliniz henÃ¼z yÃ¼kleniyor veya baÅŸlatÄ±lmadÄ±.\n\n**MesajÄ±nÄ±z:** "${message}"\n\n**Demo YanÄ±t:** Bu, Mistral 7B modelinizin yerini tutan geÃ§ici bir yanÄ±ttÄ±r. GerÃ§ek Mistral 7B modelinizi kullanmak iÃ§in:\n\n1. Model servisini baÅŸlatÄ±n: \`model_service/start_ollama_mistral.bat\`\n2. Port 8002'de servisi Ã§alÄ±ÅŸtÄ±rÄ±n\n3. Bu mesajÄ± tekrar gÃ¶nderin\n\n**Hata detayÄ±:** ${error instanceof Error ? error.message : 'Bilinmeyen hata'}\n\n*Servislerin Ã§alÄ±ÅŸÄ±p Ã§alÄ±ÅŸmadÄ±ÄŸÄ±nÄ± kontrol edin: http://localhost:8002/health*`,
        model: 'Mistral-7B-Demo-Fallback'
      };
    }
  }

  // Mistral 7B modelini Ã§aÄŸÄ±r
  private static async callMistralModel(message: string): Promise<AIResponse> {
    try {
      console.log(`ğŸ”— Making request to Mistral 7B model at: ${MISTRAL_MODEL_URL}`);
      console.log(`ğŸ“ Message: "${message}"`);
      
      const response = await axios.post(MISTRAL_MODEL_URL, {
        message: message,
        max_tokens: 1024
      }, {
        timeout: 120000, // 120 second timeout for complex questions
        headers: {
          'Content-Type': 'application/json'
        }
      });

      console.log('ğŸ“¡ Mistral 7B Model Response Status:', response.status);
      console.log('ğŸ“¡ Mistral 7B Model Response Data:', response.data);

      if (response.data && response.data.response) {
        const result = {
          content: response.data.response,
          model: response.data.model || 'Mistral-7B-Instruct-v0.3'
        };
        
        console.log('âœ… Successfully processed Mistral 7B response:', result.model);
        return result;
      } else {
        throw new Error('Invalid response structure from Mistral model service');
      }
    } catch (error) {
      console.log('ğŸ” Mistral Error Analysis:', {
        isAxiosError: axios.isAxiosError(error),
        errorCode: error instanceof Error ? (error as any).code : 'unknown',
        errorMessage: error instanceof Error ? error.message : 'unknown'
      });

      if (axios.isAxiosError(error)) {
        if (error.code === 'ECONNREFUSED') {
          throw new Error('ğŸš« Mistral 7B model service is not running on port 8002. Please start: model_service/start_mistral.bat');
        } else if (error.code === 'ECONNABORTED' || error.code === 'ETIMEDOUT') {
          // Timeout errors - don't give up immediately for complex questions
          console.log('â±ï¸ Timeout detected, but this might be a complex question. Extending wait time...');
          throw new Error('â±ï¸ Mistral is processing a complex question (this may take longer than usual)');
        } else if (error.response) {
          console.log('âŒ Mistral Service Error Response:', error.response.data);
          throw new Error(`Mistral service error (${error.response.status}): ${error.response.data?.error || error.message}`);
        } else if (error.request) {
          console.log('ğŸŒ Network request failed, but service might be running...');
          throw new Error('ğŸŒ Network communication error - please check if Mistral service is responding');
        } else {
          throw new Error(`Request setup error: ${error.message}`);
        }
      }
      throw error;
    }
  }

  // Matematik sorularÄ±nÄ± Ã¶zel olarak ele al
  private static async handleMathQuestion(message: string): Promise<AIResponse> {
    const lowerMessage = message.toLowerCase();
    
    // Basit matematik iÅŸlemleri
    const mathOperations = message.match(/(\d+(?:\.\d+)?)\s*([+\-*/])\s*(\d+(?:\.\d+)?)/);
    if (mathOperations) {
      const [, num1, operator, num2] = mathOperations;
      const a = parseFloat(num1);
      const b = parseFloat(num2);
      let result: number;
      let operation: string;

      switch (operator) {
        case '+':
          result = a + b;
          operation = 'toplama';
          break;
        case '-':
          result = a - b;
          operation = 'Ã§Ä±karma';
          break;
        case '*':
          result = a * b;
          operation = 'Ã§arpma';
          break;
        case '/':
          result = b !== 0 ? a / b : NaN;
          operation = 'bÃ¶lme';
          break;
        default:
          result = NaN;
          operation = 'bilinmeyen';
      }

      if (!isNaN(result)) {
        return {
          content: `ğŸ§® **Matematik Ã‡Ã¶zÃ¼mÃ¼:**\n\n**Soru:** ${a} ${operator} ${b}\n**Ä°ÅŸlem:** ${operation}\n**SonuÃ§:** ${result}\n\nâœ… Bu ${operation} iÅŸleminin sonucu **${result}**'dir.`,
          model: 'MathChat-Custom-v1.0'
        };
      }
    }

    // Geometri sorularÄ±
    if (lowerMessage.includes('alan') || lowerMessage.includes('Ã§evre') || lowerMessage.includes('hacim')) {
      return {
        content: `ğŸ“ **Geometri YardÄ±mcÄ±sÄ±:**\n\nGeometri sorunuz iÃ§in size yardÄ±mcÄ± olmak istiyorum!\n\n**YaygÄ±n formÃ¼ller:**\nâ€¢ Kare alanÄ±: aÂ²\nâ€¢ DikdÃ¶rtgen alanÄ±: a Ã— b\nâ€¢ Daire alanÄ±: Ï€ Ã— rÂ²\nâ€¢ ÃœÃ§gen alanÄ±: (taban Ã— yÃ¼kseklik) Ã· 2\n\nDaha spesifik bir soru sorarsanÄ±z detaylÄ± Ã§Ã¶zÃ¼m verebilirim.`,
        model: 'MathChat-Custom-v1.0'
      };
    }

    // Cebir sorularÄ±
    if (lowerMessage.includes('denklem') || lowerMessage.includes('x') || lowerMessage.includes('Ã§Ã¶z')) {
      return {
        content: `ğŸ”¢ **Cebir UzmanÄ±:**\n\nCebir sorunuzla ilgili yardÄ±m edebilirim!\n\n**Temel yÃ¶ntemler:**\nâ€¢ DeÄŸiÅŸkenleri bir tarafa topla\nâ€¢ Sabitleri diÄŸer tarafa geÃ§ir\nâ€¢ Her iki tarafÄ± da aynÄ± sayÄ±yla bÃ¶l/Ã§arp\n\nDenkleĞ¼Ğ¸Ğ½izi yazarsanÄ±z adÄ±m adÄ±m Ã§Ã¶zebilirim.`,
        model: 'MathChat-Custom-v1.0'
      };
    }

    // Genel matematik yardÄ±mÄ±
    return {
      content: `ğŸ“ **MathChat Ã–zel Modeli:**\n\nMatematik sorunuzla ilgili yardÄ±mcÄ± olmak istiyorum!\n\n**UzmanlaÅŸtÄ±ÄŸÄ±m alanlar:**\nâ€¢ Temel aritmetik iÅŸlemler (4 iÅŸlem)\nâ€¢ Geometri (alan, Ã§evre, hacim)\nâ€¢ Temel cebir (denklem Ã§Ã¶zme)\nâ€¢ YÃ¼zde hesaplamalarÄ±\n\nSorunuzu daha detaylÄ± yazarsanÄ±z size Ã¶zel bir Ã§Ã¶zÃ¼m hazÄ±rlayabilirim.`,
      model: 'MathChat-Custom-v1.0'
    };
  }

  // Genel sorularÄ± ele al
  private static async handleGeneralQuestion(message: string): Promise<AIResponse> {
    const lowerMessage = message.toLowerCase();
    
    // Selamlama
    if (lowerMessage.includes('merhaba') || lowerMessage.includes('selam') || lowerMessage.includes('hello')) {
      return {
        content: `ğŸ‘‹ **Merhaba!**\n\nMathChat Ã¶zel modeline hoÅŸ geldiniz! Ben matematik odaklÄ± bir asistanÄ±m.\n\n**Size nasÄ±l yardÄ±mcÄ± olabilirim:**\nâ€¢ Matematik problemleri Ã§Ã¶zme\nâ€¢ FormÃ¼l aÃ§Ä±klamalarÄ±\nâ€¢ AdÄ±m adÄ±m Ã§Ã¶zÃ¼m yollarÄ±\nâ€¢ Geometri ve cebir yardÄ±mÄ±\n\nBir matematik sorunuz var mÄ±?`,
        model: 'MathChat-Custom-v1.0'
      };
    }

    // Kim olduÄŸunu sorma
    if (lowerMessage.includes('kimsin') || lowerMessage.includes('nedir') || lowerMessage.includes('tanÄ±t')) {
      return {
        content: `ğŸ¤– **MathChat Ã–zel Modeli v1.0:**\n\nBen Ã¶zel olarak matematik problemleri iÃ§in eÄŸitilmiÅŸ bir yapay zeka modeliyim.\n\n**Ã–zelliklerim:**\nâ€¢ Matematik problemleri Ã§Ã¶zme\nâ€¢ AdÄ±m adÄ±m aÃ§Ä±klamalar\nâ€¢ TÃ¼rkÃ§e matematik desteÄŸi\nâ€¢ HÄ±zlÄ± hesaplamalar\nâ€¢ GÃ¶rsel formÃ¼l aÃ§Ä±klamalarÄ±\n\nMatematik konularÄ±nda size nasÄ±l yardÄ±mcÄ± olabilirim?`,
        model: 'MathChat-Custom-v1.0'
      };
    }

    // Genel yardÄ±m
    return {
      content: `ğŸ’¡ **MathChat Ã–zel Modeli:**\n\nBen matematik odaklÄ± bir asistanÄ±m. Size en iyi ÅŸekilde yardÄ±mcÄ± olabilmek iÃ§in matematik sorularÄ± sormaya odaklanmanÄ±zÄ± Ã¶neririm.\n\n**DeneyebileceÄŸiniz sorular:**\nâ€¢ "25 + 17 kaÃ§ eder?"\nâ€¢ "Dairenin alanÄ± nasÄ±l hesaplanÄ±r?"\nâ€¢ "2x + 5 = 15 denklemini Ã§Ã¶z"\n\nMatematik dÄ±ÅŸÄ±ndaki sorular iÃ§in **API Model** seÃ§eneÄŸini kullanabilirsiniz.`,
      model: 'MathChat-Custom-v1.0'
    };
  }

  // Ã–nceki mesajlarÄ± ChatMessage formatÄ±na Ã§evir
  static formatPreviousMessages(messages: any[]): ChatMessage[] {
    const formatted: ChatMessage[] = [];
    
    for (const msg of messages) {
      formatted.push({ role: 'user', content: msg.message });
      formatted.push({ role: 'assistant', content: msg.response });
    }
    
    return formatted;
  }
}

export default AIService; 